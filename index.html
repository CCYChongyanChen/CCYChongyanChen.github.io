<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="Chongyan Chen" content="notranslate">
    <meta http-equiv="Content-Language" content="en">
    <title>Chongyan Chen</title>
    <link rel="stylesheet" href="static/css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:200,300,400,500,600,700,800" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body>

    <!-- BEGIN NAVBAR -->
    <div class="navbar">
      <a href="#education">Education</a>
      <a href="#publications">Publications</a>
      <a href="#awards">Awards</a>
      <a href="#skills">Skills</a>
      <a href="#experience">Experience</a>
      <a href="#projects">Projects</a>
      <a href="#contact">Contact</a>
    </div>
    <!-- END NAVBAR -->

    <div class="container">


      <!-- BEGIN ABOUT -->
      <div class="row">
        <div class="side-content"></div>
        <div class="main-content">


                <div class="center">
                <br>
                <br>
                

                <div class="left" style="float: left; width: 300px; height: 300px;">
                    <img id="picture" src="static/images/chongyanchen.jpg" width=180 height=180>
                    <h1 margin-top = 0>Chongyan Chen</h1>                        
                    <a href="https://github.com/CCYChongyanChen">
                    <img src="static/images/github.png" width=40 height=40></a>
                    <a href="https://www.instagram.com/chongyan_chen/">
                    <img src="static/images/ins.png" width=40 height=40></a>
                    <a href="https://www.linkedin.com/in/chongyanchen/">
                    <img src="static/images/linkedin.png" width=40 height=40></a>
                    
                    


                </div>
                <div class="right" style="margin-left: 310px; height: 300px;">
                    

                        <p>Hi, I'm a first year PhD student at University of Texas at Austin. 
                          My research interest is visual question answering (VQA). 
                        Recently I am studying visual grounding and external knowledge for VQA.
                        My supervisor is Dr. Danna Gurari. Here is the link to our group: 
                        <a href="https://www.ischool.utexas.edu/~dannag/IVC_Group.html">link</a>
                        <br><br> I like drawing and reading philosophy/psychology books during my leisure time.
                        </p> 


                </div>
                
                <!-- <a href="static/links/Chongyan_Chen_CV.pdf">Here is a link to my resume</a>. -->
                </p>
            </div>
        </div>
      </div>
      <!-- END ABOUT -->

      <!-- BEGIN EDUCATION -->
      <div class="row">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="education"></a>
            <h2>Education</h2>
          </div>
          <div class="content-body">

            <!-- EDUCATION ALL -->
            <div class="card">
              <div class="card-content">
                  <ul>
                    <li>PhD, information school, University of Texas at Austin<span class="job-title">2020-2025</span></li>
                    <li>Msc, information school, University of Texas at Austin<span class="job-title">2018-2020</span><href style="float: right;">GPA: 3.96/4.0, </href></li>
                    <li>B.Eng. Electronic Engineering, South China University of Technology<span class="job-title">2014-2018</span>
                       <href style="float: right;">GPA: 3.5/4.0, </href></li>
                  </ul>
              </div>
            </div>
          </div>
        </div>
        <div class="side-content"></div>
      </div>
      <!-- END EDUCATION -->
      <!-- BEGIN PUBLICATIONS -->
      <div class="row">
      <div class="side-content"></div>
      <div class="main-content">
        <div class="content-header">
          <a class="anchor" name="publications"></a>
          <h2>PUBLICATIONS</h2>
        </div>
        <div class="content-body">
          <!-- Publication #1 -->

          <div class="project-card">
              <div class="project-image">
                <img src="static/images/IEEE_ECBIOS.png">
              </div>
              <div class="project-content">
                  <strong>"Evaluation of Mental Stress and Heart Rate Variability Derived from Wrist-Based Photoplethysmography"</strong>
                  Chongyan Chen, Chunhung Li, Chih-Wei Tsai, and Xinghua Deng. 
                  <em> IEEE Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability </em> 5/31/2019. 
                <nobr><a href="static/links/IEEE-ECBIOS-EC190063.pdf">Paper</a> | <a href="static/links/Poster_ECBIOS.pdf">Poster</a> | <a href="static/links/BestPaperAward.pdf">Award</a>  </nobr>
              </div>
          </div>

          <!-- Publication #2 -->
          <div class="project-card">
              <div class="project-image">
                <img src="static/images/ICSIP.png">
              </div>
              <div class="project-content">
                  <strong>"Activity Recognition with Wristband Based on Histogram and Bayesian Classifiers"</strong>
                  Yi-Cong Huang, Wing-Kuen Ling, Chi-Wa Cheng, Chun-Hung Li, Chong-Yan Chen. 
                  <em> IEEE 5th International Conference on Signal and Image Processing </em>(ICSIP), 7/19/2019. 
                <a href="static/links/ICSIP_2019_paper_16.pdf">Paper</a>
              </div>
            </div>


        </div>
      </div>
      <div class="side-content"></div>
    </div>
    <!-- END PUBLICATIONS -->
       <!-- BEGIN Awards -->
       <div class="row">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="awards"></a>
            <h2>AWARDS</h2>
          </div>
          <div class="content-body">

            <!-- Awards ALL -->

            <!-- Awards 1 -->
            <div class="card">
              <div class="card-header">
                <li>Kilgarlin Fellowship, <span class="job-title">2020-2024</span></li>
                <li>William and Margaret Kilgarlin Endowed Scholarship ($54,750), <span class="job-title">2020-2021</span></li>
                <li>Master Thesis-Dean’s Choice Award finalist, <span class="job-title">5/8/2020</span></li>
                <li>Best Conference Paper Award, IEEE ECBIOS, <span class="job-title">6/2019</span></li>
                <li>First Prize Award: 311 Calls and 500 Cities Hackathon, University of Texas at Austin,<span class="job-title">10/2018</span></li>
              </div>
            </div>


          </div>
        </div>
        <div class="side-content"></div>
      </div> 

      <!-- BEGIN SKILLS -->
      <div class="row">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="skills"></a>
            <h2>Skills</h2>
          </div>
          <div class="content-body">
            <ul class="ul-skills"> Programming Languages
              <br>
              <li class="li-skills">Python</li>
              <li class="li-skills">Java</li>
              <li class="li-skills">Kotlin</li>
              <li class="li-skills">C</li>
              <li class="li-skills">C++</li>
            </ul>
            <ul class="ul-skills"> Artificial Intelligence
                <br>
                <li class="li-skills">Deep Learning</li>
                <li class="li-skills">Machine Learning</li>
                <li class="li-skills">scikit-learn</li>
                <li class="li-skills">Keras</li>
                <li class="li-skills">PyTorch</li>
                <li class="li-skills">Tensorflow</li>
                <li class="li-skills"> Transfer Learning</li>
                <li class="li-skills"> Explainable AI </li>
                <li class="li-skills"> Attention mechanisms</li>
                <li class="li-skills"> Transformer</li>
                <li class="li-skills"> LSTM, BERT, BioBERT</li>
                <li class="li-skills"> CNN</li>
                <li class="li-skills"> GAN, WGAN</li>
                <li class="li-skills"> ViLBERT, MCAN</li>
            </ul>
            <ul class="ul-skills"> Web + Mobile development
                <br>
                <li class="li-skills">React Native</li>
                <li class="li-skills">Android development(Java, Kotlin)</li>
                <li class="li-skills">HTML</li>
                <li class="li-skills">CSS</li>
                <li class="li-skills">JavaScript</li>
                <li class="li-skills">PHP</li>
                <li class="li-skills">JQuery</li>
                <li class="li-skills">Ajax</li>
            </ul>
            <ul class="ul-skills"> Backend + Systems
                <br>
                <li class="li-skills">Linux</li>
                <li class="li-skills">Azure</li>
                <li class="li-skills">Google Cloud</li>
                <li class="li-skills">AWS</li>
                <li class="li-skills">Docker</li>
            </ul>
            <ul class="ul-skills"> Data related
                <br>
                <li class="li-skills"> SQL </li>
                <li class="li-skills"> NoSQL (MongoDB) </li>
                <li class="li-skills"> Qlik </li>
            </ul>
            <ul class="ul-skills"> Knowledge Graph
                    <br>
                    <li class="li-skills"> GCN </li>
                    <li class="li-skills"> Node2vec </li>
                </ul>
            <ul class="ul-skills"> CrowdSourcing
                <br>
                <li class="li-skills"> AMTurk </li>
            </ul>
            <ul class="ul-skills"> Others
                <br>
                <li class="li-skills"> Git </li>
                <li class="li-skills"> MATLAB </li>
                <li class="li-skills"> Unit Test/Jenkins </li>
                <li class="li-skills"> LaTex</li>
                <li class="li-skills"> Unity 3D </li>
            </ul>
            <ul class="ul-skills"> Design
                <br>
                <li class="li-skills"> Sketch </li>
                <li class="li-skills"> InDesign </li>
                <li class="li-skills"> Photoshop </li>
            </ul>

            <ul class="ul-skills"> Human Languages
                    <br>
                    <li class="li-skills"> Mandarin </li>
                    <li class="li-skills"> English </li>
                    <li class="li-skills"> Cantonese </li>
            </ul>

          </div>
        </div>
        <div class="side-content"></div>
      </div>
      <!-- END SKILLS -->

      <!-- BEGIN EXPERIENCE -->
      <div class="row">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="experience"></a>
            <h2>Work Experience</h2>
          </div>
        <div class="content-body">

          <!-- CARD #1 -->
            <div class="card">
              <div class="card-header">
                <h4>Algorithm Engineer (Intern), HUAWEI, Shenzhen, China, <span class="job-title">06/2019 – 08/2019</span></h4>
              </div>
                  <img src="static/images/HUAWEI.png" width=90 height=70>
              <div class="card-content">
              	<ul>
                <li>Developed the Petal Search application in the HUAWEI Consumer Business Group.</li>
	            <li>Conducted URL pattern extraction using Regex and page update detection using md5. Designed three strategies for dead links recognition and achieved 17% accuracy increment. Improved the web content extraction, obtained 11% accuracy improvement, and achieved faster speed by utilizing Hash and Dynamic Programming. 
                </li>
                <li>Reconstructed part of the XML Path Language. </li>
              	</ul>
              </div>
            </div>

          <!-- CARD #2 -->
            <div class="card">
              <div class="card-header">
                <h4>Algorithm Engineer (Intern), Add Care Ltd, Shenzhen, China<span class="job-title">11/2017 – 3/2019</span></h4>
              </div>
                  <img src="static/images/glutrac.png" width=100 height=70>
                <div class="project-content">
                    <li>Trained CNN to detect utensils in videos, used Haar-like features and Adaboost to detect human faces, and tracked them using Kernelized Correlation Filter. Identified eating gestures by collision checking between the path of utensils and human faces.</li>
                    <li>Designed stress induction experiment. Collected, filtered ECG and wrist-based PPG signals and detected signal quality. Designed Peak Finding Algorithms for PPG and ECG.</li>
                     <li>Calculated Heart Rate Variability to classify stress states. The overall Leave-One-Participant-Out accuracy of wristed-based PPG with 3 mins temporal window reaches 80%.</li>
                     <a href="https://www.add-care.net/">Add Care official website </a>
                     <a href="https://www.cnet.com/news/the-most-important-health-tech-at-ces-2020/">Glutrac- one of the best health tech at CES 2020 </a>

              
                </div>

            </div>

        

          </div>
        </div>
        <div class="side-content"></div>
      </div>
      <!-- END EXPERIENCE -->


      <!-- BEGIN PROJECTS -->
      <div class="row">
        <div class="side-content-sm"></div>
        <div class="main-content-lg">
          <div class="content-header">
            <a class="anchor" name="projects"></a>
            <h2>Projects and Research</h2>
          </div>
          <div class="content-body">
            <!-- CARD #1 -->
            <div class="project-card">
                    <div class="project-image">
                      <img src="static/images/Vizwiz.png">
                    </div>
                    <div class="project-content">
                      <h4><strong>VizWiz Research Project</strong></h4>
                      <li> <strong>VQA app: </strong>Developed a demo to capture photo and spoken question and applied speech to text (DeepSpeech) and image quality detection algorithms. (3/2019-6/2019)</li>
                      <li> <strong>Question Answerability:</strong> Extracted features for visual question’s answerability using OpenCV and Azure API; extracted text features using NLTK to predict answerability of a visual question.</li>
                      <li> <strong>Master Thesis:</strong> Answered visual question with external Knowledge (Knowledge Base, Reverse Image Search, and Image Search by Text). The results show that including external knowledge can largely improve the accuracy of VQA and show a possibility of answering questions that are labeled as unanswerable by crowd workers. (6/2019-6/2020) </li>
                      <!-- <details>
                          <summary>Click here for more details: </summary>
                          <ol>The field of computer vision has made significant advances in visual question answering (VQA) and image captioning. and image captioning Theha sophisticatedve proposed lots of fancy models, in use todaywhich works well on simple simple image captioning and VQA tasks but they perform poorly when the task requiresVQA or image captioning needs common sense or external knowledge. Previous research hasve explored Visual Question Answering (VQA) using awith knowledge base and iImage captioning usingwith reverse image search. However, there is a need for studies thatnone have explored the benefits of multi-source external knowledge for real tasks in these two areas and for real task. Besides, to our knowledge, we are the first research propose using image search by text for these two areas. 
                              <br><br>
                              This thesis compares three kinds of external knowledge: knowledgebase, reverse image search, and search by text and evaluates them on two image captioning datasets: COCO-captions and VizWiz-captions as well as on three visual question answering datasets: VQA v2, VizWiz-VQA, and OK-VQA. The results show that including external knowledge can largely improve the accuracy of VQA. This research confirms that reverse image search is suitable for the image captioning task and suggests to explore knowledge base for image captioning. It also suggests that knowledge base is more suitable for traditional VQA while search by text is more suitable for VizWiz VQA dataset. Besides, the results show a possibility of answering visual questions using low-quality images or even answering “unanswerable” questions by using external knowledge. Our research provides greater understanding for the VizWiz Challenge and reveals a gap between traditional VQA/Image captioning task and real VQA/Image captioning task from the perspective of external knowledge.
                                  
                              <img src="static/images/websites.png" width=1300>
                          </ol>
                          </details> -->
                      <li><strong>VQA tutorials:</strong> Wrote a GitHub page to summarize recent advances in VQA. Wrote articles in Zhihu (Chinese version of Quora) for VQA tutorials.</li>
                      <li><strong>VQA algorithms:</strong> applied state-of-the-art algorithms for VQA, e.g., MCAN + grid features and Pythia 3. Studying fusion methods and attention mechanisms for VQA algorithms. (6/2020-present)</li>
                      <li><strong>VQA Crowdsourcing:</strong> Building VizWiz-Visual Grounding dataset with Amazon Mechanical Turk. (9/2020-present)</li>
                      <a href="https://vizwiz.org/">VizWiz Websites</a>
                      <a href="https://www.ischool.utexas.edu/~dannag/IVC_Group.html">Our Image & Video Computing Group</a>
                    </div>
                  </div>
            <!-- CARD #X2 -->
            

            <div class="project-card">
                    <div class="project-image">
                      <img src="static/images/chestxray.png">
                    </div>
                    <div class="project-content">
                      <h4><strong>Course Project for Natural Language Generation: </strong></h4>
                      <li><strong>Chest disease classification and visual grounding:</strong> Applied hard attention model and stand-alone self-attention model to extract chest X-ray radiology images. Used multi-task learning and contrastive learning to teach model to learn from radiomics features, predict pneumonia disease, and ground the disease areas. 
                          </li>
                      <li><strong>Radiology report generation:</strong> Reproduced the paper when radiology report generation meets knowledge graph: used Densenet-121 to extract the image features and build knowledge graph via GCN and generate report via multi-level LSTM.
                          </li>      
                        </div>
                  </div>



            <!-- <div class="project-card">
                <div class="project-image">
                    <img src="static/images/chestxray.png">
                </div>
                <div class="project-content">
                    <h4>Course Project for SCIENTIF COMP MACH/DEEP LRN</h4>
                    <li>TBD</li>
                    </div>
                </div> -->
    

            <!-- CARD #X2 -->

                  

            <div class="project-card">
                    <div class="project-image">
                      <img src="static/images/KG.png">
                    </div>
                    <div class="project-content">
                      <h4><strong>Course Proj for AI in health</strong></h4>
                        <li><strong>Built COVID-19 Knowledge Graph</strong>Used BioBERT and PubTator for biomedical name entity recognition for PubMed dataset and COVID-19 44K dataset. Built coronavirus related knowledge graphs using Gephi. 
                            Integrated the coronavirus knowledge graph with the KG from Data2Discovery company. 
                        </li>
                        <li><strong>Built Tutorials: </strong>Built BioBERT tutorial and SQL tutorial for PubMed and MIMIC III dataset. Built Knowledge Graph mining algorithms tutorials.</li>
                        <nobr><a href="https://ui.adsabs.harvard.edu/abs/2020arXiv200710287C/abstract">Paper1</a> | <a href="https://www.nature.com/articles/s41597-020-0543-2">Paper 2</a> </nobr>
                            
                        
                    </div>
                  </div>



            <!-- CARD #X2 -->
            

            <div class="project-card">
                    <div class="project-image">
                      <img src="static/images/byteMe.png">
                    </div>
                    <div class="project-content">
                      <h4><strong>Course Proj for Advanced Programming Tools: ByteMe</strong></h4>
                      <li>Developed ByteMe application for both Web (frontend: HTML + JS + Ajax; backend: Python + Flask) and Mobile platforms (React Native and Kotlin).</li>
                      <li>Built, deployed and managed application using Google App Engine, wrote python Database API to handle MongoDB, developed navigation function, camera function, and user login function with Google Firebase.</li>
                      <li>Implemented “NewByte” page with the AutoFill function using Food 101 classification model based on Google Inception V3 model and Azure API. </li> 
                      <a href="https://drive.google.com/file/d/1OjuPIglZxhqDGUA27wXBEEydIehKnoCi/view">Website</a><a href="https://youtu.be/phnG5iLbYHw">App made by Kotlin</a> <a href="https://www.youtube.com/watch?v=U5gBgHM0Emg">App made by React Native</a>    
                  </div>
                  </div>




                  <div class="project-card">
                        <div class="project-image">
                          <img src="static/images/IEEE_ECBIOS.png">
                        </div>
                        <div class="project-content">  
                            
                      <h4><strong>Research Proj: Evaluation of Mental Stress and Heart Rate Variability Derived from Wrist-Based Photoplethysmography</strong></h4>
                            <li>Designed stress induction experiment. Collected, filtered ECG and wrist-based PPG signals and detected signal quality. Designed Peak Finding Algorithms for PPG and ECG.</li>
                            <li>Calculated Heart Rate Variability to classify stress states. The overall Leave-One-Participant-Out accuracy of wristed-based PPG with 3 mins temporal window reaches 80%.</li>
                          <nobr><a href="static/links/IEEE-ECBIOS-EC190063.pdf">Paper</a> | <a href="static/links/Poster_ECBIOS.pdf">Poster</a> | <a href="static/links/BestPaperAward.pdf">Award</a>  </nobr>
                        </div>
                    </div>
    

            <!-- CARD #3 -->
            <div class="project-card">
              <div class="project-image">
                <img src="static/images/eye_tracking.png">
              </div>
              <div class="project-content">
                <h4><strong>Course Proj for HCI: Understanding Health-related Information Searching Behavior Through Eye Tracking </strong></h4>
                <p>
                Collected eye-tracking data (AOIs, TTFF, etc.) using Tobii TX300 eye-tracker and iMotions.
                Analyzed data using Kruskal-Wallis test, One-Way Anova and Mann-Whitney U Test. (Paper)
              </p>
               <nobr> <a href="static/links/eye-tracking-paper.pdf">Paper</a> |
                <a href="static/links/eye-tracking-poster.pdf">Poster</a></nobr>
              </div>
            </div>
            <!-- CARD #5 -->
            <div class="project-card">
              <div class="project-image">
                <img src="static/images/activity_recognition.png">
              </div>
              <div class="project-content">
                <h4><strong>Course Proj for Activities Recognition: Activities Recognition in Self-Driving Car</strong></h4>
                <p>
                  Collected ten peoples’ five activities to solve the take-over problem. Reduced individual differences. Built pose estimator to detect skeleton of people. Extracted secondary features to help classify similar activities. Ensemble them with LSTM. (Paper)
                </p>
                <br>
                <a href="static/links/Activity_Recognition_Final_Paper.pdf">Paper</a>
              </div>
            </div>
            
     <!-- CARD #6 -->
            <div class="project-card">
                    <div class="project-image">
                    <img src="static/images/VR.png">
                    </div>
                    <div class="project-content">
                    <h4><strong>Course Proj for Visual Environment</strong></h4>
                    <li>Summary: We used Unity 3D to build a virual presentation demo.</li>
                    <li>Why: Our design can help people with presentation anxiety and improve presentation skills. It also provide a solution for distance meeting.
                    </li>
                    <li>Details: We disigned different human-human interaction/attitudes for virtual audience. For positive attitude, some virtual audience would imitate the actions of the speakers when the speaker is doing experiment, some audience would always pay attention to the speaker by turning their body towards the speaker.
                            For passive attitude, the audience just ignore the speaker.
                        <br>Besides, we designed different human-objects interactions: interacting with slides, poping out details of the display item when user gets close to the item, etc.
                    </li>
                    </div>
                </div>

            <!-- CARD #6 -->
            <div class="project-card">
                    <div class="project-image">
                      <img src="static/images/Self_Driving_MCM.png">
                    </div>
                    <div class="project-content">
                      <h4><strong>Research Proj: 2017 Mathematical Contest in Modeling - "Cooperate and navigate"</strong></h4>
                      <li>Summary: We analyzed of the effects of allowing self-driving, cooperating cars on the roads in several countries in the U.S. as well as suggesting the best percentage of self-driving car, and policy changes like setting exclusive lane.</li>
                      <li>Why: Self-driving, cooperating cars have been proposed as a solution to increase capacity of highways
                            without increasing number of lanes or roads. The behavior of these cars interacting with the existing
                            traffic flow and each other is not well understood at this point</li>
                      <li>Details: We built Phantom Traffic Jam Model to simulate traffic jam on highway with few intersections and accidents. Created Smart Driver Model with versions for human drivers and smart cars.
                          <br>
                          We predicted traffic condition with varied road density and smart car proportions.
                      <br>We built Global Decision Model to control smart car proportions and provide optimal route plans for both human drivers and smart cars.
                      <a href="static/links/MCM.pdf">Paper</a>
                    </div>
                  </div>


            <!-- CARD #X
            <div class="project-card">
              <div class="project-image">
                <img src="http://placehold.it/250x350">
              </div>
              <div class="project-content">
                <h4>Project Name</h4>
                <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
                <a href="#">Link Text</a>
              </div>
            </div>-->

          </div>
        </div>
        <div class="side-content-sm"></div>
      </div>
      <!-- END PROJECTS -->

      <!-- BEGIN SKILLS
      <div class="row">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="skills"></a>
            <h2>Skills</h2>
          </div>
          <div class="content-body">
            <ul class="ul-skills">
              <li class="li-skills">Python</li>
              <li class="li-skills">C</li>
              <li class="li-skills">C++</li>
              <li class="li-skills">Java</li>
              <li class="li-skills">Bash</li>
              <li class="li-skills">Docker</li>
              <li class="li-skills">Pandas</li>
              <li class="li-skills">Javascript</li>
            </ul>
          </div>
        </div>
        <div class="side-content"></div>
      </div>
      END SKILLS -->

      <!-- END EXPERIENCE -->

      <!-- BEGIN PERSONAL
      <div class="row white box-shadow">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="about"></a>
            <h2>Personal Life</h2>
          </div>
          <div class="content-body">
            <p>I'm very into music. </p>
            <p>Outisde of my career interests, I have multiple interests in the humanities. I'm a history minor and I love to play the guitar. I'm also a debate coach. </p>
          </div>
        </div>
        <div class="side-content"></div>
      </div>
      END PERSONAL -->

      <!-- BEGIN CONTACT -->

      <div class="row blue">
        <div class="side-content"></div>
        <div class="main-content">
          <div class="content-header">
            <a class="anchor" name="contact"></a>
            <h2 class="white-text">Contact Me</h2>
          </div>
          <div class="contact content-body">
            <form method="POST" action="https://formspree.io/chongyanchen_hci@utexas.edu">
                <input type="hidden" name="_subject" value="Contact request from personal website" />
                <input type="email" name="_replyto" placeholder="Your email" required>
                <textarea name="message" placeholder="Your message" required></textarea>
                <button type="submit">Send</button>
            </form>
          </div>
        </div>
        <div class="side-content"></div>
      </div>
      <!-- END CONTACT -->




    </div>
  </body>
</html>
