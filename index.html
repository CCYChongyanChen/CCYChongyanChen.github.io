<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="Chongyan Chen" content="notranslate">
    <meta http-equiv="Content-Language" content="en">
    <title>Chongyan Chen    陈崇彦</title>
    <link rel="stylesheet" href="static/css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:200,300,400,500,600,700,800" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body>

    <!-- BEGIN NAVBAR -->
    <div class="navbar">
      <a href="#news">News</a>
      <a href="#publications">Publications</a>
      <a href="#education">Education</a>
      <a href="#awards">Awards</a>
    </div>
    <!-- END NAVBAR -->

    <div class="container">


      <!-- BEGIN ABOUT -->
      <div class="row">
        <div class="side-content"></div>
        <div class="main-content">


                <div class="center">
                <br>
                <br>
                

                <div class="left" style="float: left; width: 300px; ">
                    <img id="picture" src="static/images/chongyanchen.jpg" width=180 height=180>
                    <br>
                    <a href="https://github.com/CCYChongyanChen">
                    <img src="static/images/github.png" width=40 height=40></a>
                    <a href="https://www.instagram.com/chongyan_chen/">
                    <img src="static/images/ins.png" width=40 height=40></a>
                    <a href="https://www.linkedin.com/in/chongyanchen/">
                    <img src="static/images/linkedin.png" width=40 height=40></a>
                    
                    


                </div>
                <div class="right" style="margin-left: 310px; ">
                    
                    <h1 margin-top = 0>Chongyan Chen  陈崇彦</h1>                        

                        <p>Hi there! I'm a fourth-year Ph.D. student at UT Austin, advised by Prof. Danna Gurari. My research centers on inclusive and accessible visual question answering in authentic use cases.  Currently, I am exploring visual questions that require lengthy answers that can accommodate diverse perspectives. Generally, I take great pleasure in redefining ill-posed problems, delving into data to discover insights, and designing algorithms to address challenges.


                        <br><br>In my free time, I enjoy drawing. I also write science articles on social media, and I'm the founder of a vision and language discussion group with over 200 researchers who have joined.
                        </p> 


                </div>
                
                <!-- <a href="static/links/Chongyan_Chen_CV.pdf">Here is a link to my resume</a>. -->
                </p>
            </div>
        </div>
      </div>
      <!-- END ABOUT -->

            <div class="row">
            <div class="side-content"></div>
            <div class="main-content">
                <div class="content-header">
                    
                    <!-- BEGIN NEWS -->
                    <a class="anchor" name="news"></a>
                    <h2>NEWS</h2>
                    <div class="card">
                        <div class="card-header">
                                
                                <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                                    <colgroup>
                                        <col width="15%">
                                        <col width="85%">
                                    </colgroup>
                                    <tbody>
                                        
                                        
                                        <tr>
                                            <td valign="top" align="center"><strong>09/2024</strong></td>
                                            <td>Our paper <a href = "https://minahuh.com/LFVQA/">VizWiz-LF</a> is selected for an oral Presenation (Top 2% of 1036 submissions)!
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>07/2024</strong></td>
                                            <td>Our paper <a href = "https://minahuh.com/LFVQA/">VizWiz-LF</a> is accepted by COLM 2024!
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>07/2024</strong></td>
                                            <td>Our paper <a href = "https://vqaonline.github.io/">VQAonline</a> is accepted by ECCV 2024! We introduce the first fully authentic VQA dataset with long-form answers!  
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>06/2024</strong></td>
                                            <td>Organizing the <a href="https://vizwiz.org/workshops/2024-vizwiz-grand-challenge-workshop/">VizWiz Grand Challenge Workshop</a>  at CVPR 2024
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>05/2024</strong></td>
                                            <td>Started my internship with Google DeepMind! 
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>07/2023</strong></td>
                                            <td>Our paper is accepted by ICCV 2023!
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>06/2023</strong></td>
                                            <td>Organizing the <a href="https://vizwiz.org/workshops/2023-workshop/">VizWiz Grand Challenge Workshop</a>  at CVPR 2023
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>01/2023</strong></td>
                                            <td>Started my internship with the Microsoft Azure Computer Vision Team
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>03/2022</strong></td>
                                            <td>Our CVPR paper is selected for an oral presentation!
                                            </td>
                                        </tr>
                                        <tr>
                                            <td valign="top" align="center"><strong>03/2022</strong></td>
                                            <td>Our paper is accepted by CVPR 2022!
                                            </td>
                                        </tr>
                                    <tr>
                                        <td valign="top" align="center"><strong>02/2022</strong></td>
                                        <td>Organizing the first <a href="https://eval.ai/web/challenges/challenge-page/1529/overview"> VizWiz-VQA-Answer Grounding Challenge</a> at the
                                            <a href="https://vizwiz.org/workshops/2022-workshop/">VizWiz Grand Challenge Workshop</a>
                                        </td>
                                    </tr>
                                    </tbody>
                                </table>            
                        </div>
                        </div>
                        <!-- END NEWS -->
            <!-- BEGIN PUBLICATIONS -->
                <a class="anchor" name="publications"></a>
                <h2>PUBLICATIONS</h2>
                
                <div class="content-body">
                    
                <!-- Publication #2 -->
                    

                <div class="project-card">

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                            <td width="35%">
                                <div class="one">
                                    <div class="two" id="savvn"><img src="static/images/main_figure3.png" alt="sym" width="105%" style="border-style: none"></div>
                                </div>
                            </td>
                            <td valign="top" width="65%">
                                <p><br>
                                    <!-- <img src="./Changan Chen_files/new.png" alt="[NEW]" width="5%" style="border-style: none"> --><a href ="https://arxiv.org/pdf/2311.15562.pdf">
                                    <heading>Fully Authentic Visual Question Answering Dataset from Online Communities</heading>
                                </a><br><strong>Chongyan Chen</strong>, Mengchen Liu, Noel Codella, Yunsheng Li, Lu Yuan, and Danna Gurari.<br> 
                                  <a href="https://vqaonline.github.io/">VQAonline Homepage</a>, 
                                  <em>ECCV, 2024<br></em>
                            </p>
                            <br> 
                        </td>
                        </tr>
                    </tbody></table>
                    </div>
                <div class="project-card">

                <div class="project-card">

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                            <td width="35%">
                                <div class="one">
                                    <div class="two" id="savvn"><img src="static/images/grounding_difference2.png" alt="sym" width="100%" style="border-style: none"></div>
                                </div>
                            </td>
                            <td valign="top" width="65%">
                                <p><br>
                                    <!-- <img src="./Changan Chen_files/new.png" alt="[NEW]" width="5%" style="border-style: none"> --><a href ="https://arxiv.org/abs/2308.11662">
                                    <heading>VQA Therapy: Exploring Answer Differences by Visually Grounding Answers</heading>
                                </a><br><strong>Chongyan Chen</strong>, Samreen Anjum, Danna Gurari.<br> 
                                    <em>ICCV, 2023<br></em>
                            </p>
                            <br> 
                        </td>
                        </tr>
                    </tbody></table>
                    </div>
                <div class="project-card">

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                        <tr>
                        <td width="35%">
                            <div class="one">
                                <div class="two" id="savvn"><img src="static/images/vizwizheader.png" alt="sym" width="100%" style="border-style: none"></div>
                            </div>
                        </td>
                        <td valign="top" width="65%">
                            <p> <span style="background-color:#ec2a1c; color:azure;font-family:sans-serif;font-size:small">&nbsp;Oral presentation (Top 5% of 8000+ submissions)</span> <br><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Grounding_Answers_for_Visual_Questions_Asked_by_Visually_Impaired_People_CVPR_2022_paper.html">
                                <!-- <img src="./Changan Chen_files/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                                <heading>Grounding Answers for Visual Questions Asked by Visually Impaired People</heading>
                            </a><br><strong>Chongyan Chen</strong>, Samreen Anjum, Danna Gurari.<br> 
                                <em>CVPR, 2022<br></em>
                        </p>
                        <br> 
                    </td>
                    </tr>
                </tbody></table>
                </div>







                <div class="project-card">

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                        <tr>
                        <td width="35%">
                            <div class="one">
                                <div class="two" id="savvn"><img src="static/images/KG_contrastive_learning_WACV.png" alt="sym" width="100%" style="border-style: none"></div>
                            </div>
                        </td>
                        <td valign="top" width="65%">
                            <p><a href="https://openaccess.thecvf.com/content/WACV2022/html/Han_Knowledge-Augmented_Contrastive_Learning_for_Abnormality_Classification_and_Localization_in_Chest_WACV_2022_paper.html">
                                <!-- <img src="./Changan Chen_files/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                                <heading>Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-Rays With Radiomics Using a Feedback Loop</heading>
                            </a><br>Yan Han, <strong>Chongyan Chen</strong>, Ahmed Tewfik, Benjamin Glicksberg, Ying Ding, Yifan Peng, Zhangyang Wang;<br>
                                <em>WACV, 2022<br></em>
                        </p>
                    </td>
                    </tr>
                </tbody></table>
                </div>
                <div class="project-card">

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                        <tr>
                        <td width="35%">
                            <div class="one">
                                <div class="two" id="savvn"><img src="static/images/IEEE_ECBIOS.png" alt="sym" width="100%" style="border-style: none"></div>
                            </div>
                        </td>
                        <td valign="top" width="65%">
                            <p><span style="background-color:#ec2a1c; color:azure;font-family:sans-serif;font-size:small">&nbsp;Best Paper Award </span> <br><a href="static/links/IEEE-ECBIOS-EC190063.pdf">
                                <!-- <img src="./Changan Chen_files/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                                <heading>Evaluation of Mental Stress and Heart Rate Variability Derived from Wrist-Based Photoplethysmography</heading>
                            </a><br><strong>Chongyan Chen </strong>, Chunhung Li, Chih-Wei Tsai, and Xinghua Deng;<br>
                                <em>IEEE ECBIOS, 2019<br></em>
                                
                        <a href="static/links/Poster_ECBIOS.pdf">Poster</a> | <a href="static/links/BestPaperAward.pdf">Award</a> 
                        </p>
                    </td>
                    </tr>
                </tbody></table>
                </div>




                </div>


            <!-- END PUBLICATIONS -->
            
            

            <!-- BEGIN EDUCATION -->


                <a class="anchor" name="education"></a>
                <h2>Education</h2>

                <!-- EDUCATION ALL -->
                <div class="card">
                <div class="card-header">
                        <li>PhD, information school, University of Texas at Austin<span class="job-title">2020-2025</span></li>
                        <li>Msc, information school, University of Texas at Austin<span class="job-title">2018-2020</span><href style="float: right;"> </href></li>
                        <li>B.Eng. Electronic Engineering, South China University of Technology<span class="job-title">2014-2018</span>
                        <href style="float: right;"> </href></li>
                </div>
                
            </div>

        <!-- END EDUCATION -->
        


            <!-- END AWARD -->
            <a class="anchor" name="awards"></a>
            <h2>AWARDS</h2>


                <div class="card">
                    <div class="card-header">
                        <li>Kilgarlin Fellowship, <span class="job-title">2020-2024</span></li>
                        <li>William and Margaret Kilgarlin Endowed Scholarship, <span class="job-title">2020-2021</span></li>
                        <li>Master Thesis-Dean’s Choice Award finalist, <span class="job-title">5/8/2020</span></li>
                        <li>Best Conference Paper Award, IEEE ECBIOS, <span class="job-title">6/2019</span></li>
                        <li>First Prize Award: 311 Calls and 500 Cities Hackathon, University of Texas at Austin,<span class="job-title">10/2018</span></li>
                    </div>
                    </div>
            </div>






            
            <div class="side-content"></div>
            </div>






      
    
    
    </div>
</body>
</html>
