Chest X-ray Radiology Report Generation with Knowledge Graph
Chongyan Chen
Information school
University of Texas at Austin
chongyanchenhci@utexas.edu
Abstract
Generating a radiology report automatically
can alleviate the workload of the radiologists.
Though Deep Learning (DL) has shown some
potential in helping radiology report generation, due to the imbalanced and limited dataset,
generating abnormalities reports still remain
to be a hard problem. We reproduced the paper (Zhang et al., 2020), which utilized the
densenet-121 and graph convolutional neural
network for dedicated feature learning and
relationship modeling between each feature
and used multi-level LSTM for report generation. We showed that, though achieving
state-of-the-art accuracy, their model overfits
greatly. To address their problem and to force
the model to learn more about abnormalities,
we weighted the binary cross-entropy for abnormalities classification and added supplementary experiments that trained only on abnormalities for report generation. Also, we
demonstrated the attention map over the graph
to show which topic each sentence is focusing on. Our results show great performance
on the Indiana University Chest X-Ray Collection (IU X-Ray) dataset. Besides, our experiments show less overfitting and provide more
explainability.
1 Introduction
Generating a radiology report automatically could
aid radiologists’ decision-making process. In recent years, artificial intelligence has achieved great
performance in image captioning area (Chen et al.,
2015; Gurari et al., 2020; Wang et al., 2016; Zhou
et al., 2019) and has adapted to radiology report
generation (Shin et al., 2016; Xue et al., 2018;
Wang et al., 2018; Li et al., 2018; Jing et al., 2017;
Zhang et al., 2020). Radiology image generation
task is different from general image captioning task
in two aspects: (1) using the exact biomedical keywords is essential in radiology image generation
while image captioning task has a wider word bags.
(2) The evaluation metrics for radiology report generation should focus more on measuring how match
the keywords while popular evaluation metrics of
image captioning tasks are based on N-gram. In
addition, previous work of radiology report generation may focus on describing the report from different perspectives, but didn’t model the relationship
between each observation. Also, generating radiology report for abnormal cases are much harder than
generating for normal cases due to the imbalanced
dataset.
The main method we use is mainly based on
(Zhang et al., 2020), which uses Densenet-121
model to extract features from the radiology images.
The extracted features are served as initial nodes
features for our radiology abnormalities graph and
GCN is used for update the node features. In the
graph, the nodes are disease findings. The related
findings are connected. Then a multi-level (topic
level and a word level) decoder module based on
LSTM is used for report generation.
The contributions of our work are three folds:
(1) We showed the drawbacks of the original
experiments. The original experiments are trained
on both normal and abnormal cases. However, due
to the very imbalanced data and unsuitable learning rate, their model only performs well on normal cases, though achieved state-of-the-art AUC of
ROC. We also demonstrate the needs of abnormalities classification before report generation.
(2) To deal with the imbalanced dataset, we
weighted the loss function for abnormalities classification and added an experiment that only train on
abnormalities cases for report generation.
(3) We visualized the attention map over the
graph to indicate which topic each sentence is focusing on, which provides better explainability.
2 Related Work
A report generally consists of sections like comparison, indication, findings, and impression. Findings
provide detailed observations of the image while
the impression is the summary of the observations.
(Monshi et al., 2020) surveyed the datasets and
DL algorithms for generating radiology report.
Publicly available radiology images and associated reports datasets include IU X-Ray, ChestXray, CheXpert, and MIMIC-CXR. They categorized SOTA algorithms into 3 categories: word
(Wang and Xia, 2018; Monshi et al., 2019), sentence (Shin et al., 2016; Kisilev et al., 2016; Jing
et al., 2017), and paragraph-level (Xue et al., 2018;
Wang et al., 2018; Li et al., 2018; Jing et al., 2017).
The “word-level radiology report generation” is a
disease classification problem. In our work, we are
aiming for paragraph-level report generation.
(Wang et al., 2018) Incorporating knowledge
graph allows more dedicated feature learning and
relationship modeling. (Zhang et al., 2020) propose
to use a pre-constructed graph embedding module
on multiple disease findings to generate radiology
report. They tested their dataset on the IU-RR of
chest radiographs. (Li et al., 2019) proposed the
Knowledge-driven Encode, Retrieve, Paraphrase
(KERP) approach. The encode module constructs a
medical abnormality graph based on visual features.
The Retrieve module retrieves text templates and
the Paraphrase module rewrites the templates.
Classic evaluation metrics for image captioning including BLEU(Papineni et al., 2002),
ROUGE(Lin, 2004), and CIDEr(Vedantam et al.,
2015), METEOR(Elliott and Keller, 2013). Among
them, BLEU, ROUGE, and CIDEr are based on
n-gram, which means in the matching, each word
matters equally. However, this is not accurate especially in medical image reporting area for two
reasons: (1) biomedical terms should be valued
more than other words, like stop words. (2) Radiologists may use some words like “without”, “no” to
exclude diseases, say “Has pneumothorax and large
pleural effusion” and “no pneumothorax or large
pleural effusion” are expressing totally opposite
meaning while their BLEU, ROUGE, and CIDEr
score are both high. (Zhu et al., 2018) includes five
text generation metrics, including BLEU, EmbSim,
NLL-oracle, NLL-test, and Self-BLEU.
Few researchers design evaluation metrics for
radiology reports: (Xue et al., 2018) proposed to
use keywords accuracy, which can address the first
problem. (Zhang et al., 2020) proposed the Medical Image Report Quality Index (MIRQI). (Liu
et al., 2019) proposed a report generating system
and fine-tuned it using reinforcement learning, considering both readabilities, clinical accuracy, and
proposed Clinically Coherent Reward, which aims
to maximize the correlation of distribution over disease states between the generated text and ground
truth text. Both MIRQI and CCR consider biomedical keywords and positive/negative words. Also,
human/medical expert evaluation is the most expensive but most accurate way to evaluate. In other
area, (Sellam et al., 2020) proposed to use BERT to
model human judgments to build a learned evaluation metric. Some researchers conduct a qualitative
evaluation on radiology report generation tasks by
measuring content coverage, length, medical term,
accuracy, and text fluency.
(Zhu et al., 2018)’s Texygen library includes several text generation models based on GAN, including Vanilla MLE, SeqGAN, MaliGAN, RankGAN,
GSGAN, TextGAN, and LeakGan. Our work aims
to generate both findings and impressions. The
most similar work to us is (Zhang et al., 2020),
which uses multi-layer LSTM for report generation.
Our work is different from previous work. Instead of proposing new method, we revisited the
method proposed by (Zhang et al., 2020) and illustrated that their method over-fits badly with unsuitable learning rate setting. We proposed to adjust
loss function for classification and separately train
the normal cases and abnormal cases for generation
to avoid overfitting. Besides, we illustrated how the
topic level LSTM select which topic to focus on
by showing the attention over graph. This provides
evidence for adjusting the models.
3 Dataset
We conducted experiments on Indiana University
Chest X-Ray Collection (IU X-Ray) (DemnerFushman et al., 2016), which consists of 7,470
chest x-ray images associated with 3,955 radiology reports. We select this dataset because it is the
most popular dataset for radiology report generation (Shin et al., 2016; Xue et al., 2018; Wang et al.,
2018; Li et al., 2018; Jing et al., 2017; Zhang et al.,
2020). For each patient, 2 chest x-ray is provided:
a frontal view image and a lateral view image. The
Associated radiology reports contains impression,
finding, and indication sections. For all our ex-
periments, we only use the cases which have both
frontal and lateral views with both impressions and
findings, thus we only have 2902 cases in total. The
image size is 512 × 512. No data agumentation is
used for our experiments.
4 Methods
Evaluation Metrics For abnormalities classification, we reported test precision, recall, F1, and
AUC of ROC curve. We use traditional evaluation
metrics for generation evaluation: BLEU1, BLEU2,
BLEU3, BLEU4, CIDEr, ROUGE, and loss. We
also manually evaluate the results.
4.1 Model structure
Figure 1 shows the pipline of the method we propose:
As shown , images of both front and lateral views
are the input to the densenet-121 model and then
an attention mechanism to initialize node features
for radiology knowledge graph. The nodes and
the edges for the radiology knowledge graph are
hard-coded according to prior knowledge. The
node features are then updated via graph convolution network. Then a multi-label classification is
built to predict 20 radiology related classes. The
Densenet-GCN-Classification is stage 1. We fixed
the weights of densenet and GCN after stage 1 and
trained a two-layer LSTM for report generation
after an attention mechanism over the graph.
4.2 Visual Feature Extraction.
Densenet-121 is pretrained on ImageNet. The output of the Densenet-121 is a 1024×16×6 feature
maps.
4.3 Graph Construction.
Nodes and edges Initialization The graph is initialed with 21 nodes. Twenty of them are the most
common abnormalities in clinical radiology reports
and one is a global node. We grouped it by the body
part that they belong to apart from node ‘normal’,
‘other’, and ‘foreign object’ based on the fact that
abnormalities. Figure 2 demonstrates the graph
nodes and edges. Nodes in grey are virtual and just
indicate group categories. The edges of the graph
are connected bidirectional and the global node is
connected to all other nodes.
Node Feature Initialization The node features
are initialized via an attention module after
DenseNet-121. The weights of the attention maps
are obtained via a softmax layer after a convolutional layer with a 1 x 1 kernel, see equation 1.
There are 20 channels after the 1 x 1 convolutional
layer, each channel represents a class or finding,
which means each channel learn a different location to look. The global node feature is initialized
by the average pooling after DenseNet-121. Then
we get the initialized node features by multiplying
the attention weights with the original features.
attention weights = softmax (conv (feats)) (1)
Update Node Features via Graph convolution
Graph convolution can be used to update node features for the knowledge graph. Generally, graph
convolution can be defined as Equation (2), where
S
l
and S
l+1 denotes node states in the l-th and
l + 1 layer, respectively. We defined message as
Equation (4) and thus get Equation (5). A is the
normalized Laplacian of the adjacency matrix for
our predefined graph, shown in Figure 2. message
contains both forward message as well as backward message generated by node statesl
and the
forward and backward normalized Laplacian adjacency matrix A. The next state S
l+1 is generated
by current condensed S
l
and m through a 1 d convolution layer. Batch Normalization BN, RELU,
and residual connections are added after the 1 d
convolution layer. We have three Graph convolutional layers in total.
F
l+1 = update 
F
l
, message 
F
l
, A (2)
Condensed S = ReLU 
BN 
Conv 1d

S
l

(3)
m = ReLU 
BN 
Conv 1d

S
l

A
 (4)
S
l+1 = ReLU BN (Conv 1d (concat (condensed S, m))) + S
l

(5)
4.4 Abnormalities Classification
A global average pooling, a fully convolution layer
with sigmoid function is added after GCN to predict the 20 abnormalities classes. The loss function
is binary cross-entropy loss. Since the dataset is imbalanced, the binary cross-entropy loss is weighte
Figure 1: Overview of our proposed framework. The densenet-121 model is used to extract node features, GCN is
used to update the node features and a multi-level LSTMs are used for report generation.
Figure 2: Predefined radiology graph. 20 of the nodes are the top 20 most common abnormalities in clinical note
and one is the global node.
according to the distribution of the training dataset.
We predict a class as ‘1’ if the confidence score is
larger than 0.5.
4.5 Report Generation.
We use a two-layer LSTM for report generation,
one topic level generation and another one is wordlevel generation. It is inspired by the observation
that each sentence in clinical reports often focuses
on a different topic.
4.6 Topic level Generation.
The input to the topic level LSTM is a weighted
context obtained by node states through an attention mechanism over the graph, which makes each
sentence focus on different findings.
Attention over Graph Equation (6)(7)(8) shows
how we get the context from graph embeddings. ei
denotes a node state. ai
is the attention weight on
each node i. vt
is the context vector. Overall, the attention mechanism, given what has been predicted,
will predict which topic(node) to focus on for the
next sentence.
ai = Wa tanh (Wvei + Wshs,t−1) (6)
αi = softmax (ai) (7)
vt =
X
i
αiei (8)
4.7 Word level Generation.
The word level LSTM take the context vector vt
and the topic vector st obtained from the topic-level
LSTM and generate report word-by-word, shown
by following equations, where i, f, o denotes input
gate, forget gate, and output gate. w denotes word
and τ denotes time step. c is the cell state and h is
the hidden state.
iw,τ = σ (Wsist + Wvivt + Whihw,τ−1)
fw,τ = σ (Wsf st + Wvf vt + Whfhw,τ−1)
gw,τ = tanh (Wsgst + Wvgvt + Whghw,τ−1)
ow,τ = σ (Wsost + Wvovt + Whohw,τ−1)
cw,τ = fw,τ ∗ cw,τ−1 + iw,τ ∗ gw,τ
hw,τ = ow,τ ∗ tanh (cw,τ )
(9)
4.8 Experiment set up
For stage 1 abnormalities classification, we first
trained the model with a learning rate of 1e-6 for
155 epochs following the original paper. This leads
to a low training loss but very high test loss. Thus
we set other three different learning rate, that is
1e-3, 1e-4, 1e-5 and decay the learning rate by 0.1
at epoch 30, 50, and 100. For stage 1, the optimizer
is Adam with the weight decay 1e-5. We showed
our results in Figure 3.
According to Figure 3 and the training and testing loss we get, we finally selected two models for
report generation, one is with learning rate equals
to 1e-5 and stopped it at 135 epochs and another
one is lr = 1e-4 and early stopped at 5 epochs. We
hypothesis that it is not necessary to train for that
large epochs due to the limited dataset.
For stage 2 training we set the decoder learning
rate as 1e-4. We decayed th learning rate by 0.1 at
epoch 30, 50, and 100 also. We trained the model
for 100 epochs.
Class Name F1-ours F1-ori
Normal 0.642 0.525
Cardiomegaly 0.339 0.234
Scoliosis / degenerative 0.306 0.236
fractures bone 0.143 0.136
pleural effusion 0.302 0.2
thickening 0 0
pneumothorax 0 0
hernia hiatal 0.143 0.070
calcinosis 0.161 0.067
emphysema 0.2 0.170
pneumonia 0.155 0.122
pulmonary edema 0 0
pulmonary atelectasis 0.319 0.2
cicatrix 0.205 0.163
opacity 0.365 0.280
nodule/mass 0.031 0.015
airspace disease 0.182 0
hypoinflation 0.382 0.249
catheters indewelling 0.268 0.174
other 0.187 0.128
Table 1: Results for abnormalities classification
4.9 Deal with imbalanced data
In addition to the weighted loss function, we additionally added an experiment that only trained for
abnormal cases to prevent the model only learn to
generate reports for normal cases and don’t learn
anything for abnormal cases.
5 Results
5.1 Results for abnormalities classification
The paper (Zhang et al., 2020) we are following are
pretrained on CheXpert dataset while ours didn’t.
We reported the results for abnormalities classification in Table 1. As shown, the model learns to
classify the normal cases well but fail for thickening, pneumothorax, and pulmonary edema. Compared to other cases, these three have relatively
small areas on the images, which may lead to its
bad performance. For all the cases, our model
achieved higher F1 score compared to (Zhang et al.,
2020) and get higher test recall score for most of
the classes. Comparing to other experiments, our
improvement is around 2% for AUC score which
may come from message passing through GCN,
allowing the interactions between each finding.
Figure 3: Selecting Learning Rate for abnormalities classification.
5.2 Results for Report Generation
We reported the results for report generation in
Table 2. Comparing SentSAT+KG without classification and with classification, we noticed that
without classification, the model actually have a
higher metric scores. However, when manually
going through the generated cases, we found that
without classification, the model only generate reports that indicate no disease.
5.3 Feature Map
We visualize the attention over graph in Figure 4
for normal case shown by Figure 5. The x axis
is the node/finding names. The y axis is the i-th
sentence. The upper attention map is generated by
the frontal view and the bottom is generated by the
lateral view. As shown, for the frontal view, the
model focuses more on fractures bone for the first
sentence and cardiomegaly for the second sentence.
For the lateral view, the model focuses more on
scoliosis for the second sentence. We found that
the model doesn’t focus on the hypothesized areas
for both normal and abnormal cases. Moreover,
after 4-th sentences, the model attention remains
the same.
Qualitative evaluation Figure 5 shows two examples, one is normal case and another one is abnormal case. For the abnormal case, we showed the
reports generated from two models, one is trained
and tested on both normal and abnormal cases and
another is trained only on the abnormal cases. As
shown, the normal case is perfectly matched with
the ground truth.
For the abnormal case, the ground truth indicates ‘sternotomy’ and ‘calcifications of the thoracic aorta. The model trained on both normal and
abnormal cases can also successfully predict ‘sternotomy’ and ‘calcified granulomas’. However, it
wrongly predicts the patient has mild cardiomegaly.
Besides, notice that ‘there are calcified granular’
are predicted twice.
We also trained the model only on the abnormal
cases. We got the abnormal cases by discarding
all cases with normal label =1 for the classification
task. Due to the limited time, we only have trained
it for 20 epochs. We hypothesis that at this stage, it
only learns to generate the most common sentences
in report with abnormalities.
Method BLEU-1 B-2 B-3 B-4 CIDEr ROUGE
CoAtt 0.455 0.288 0.205 0.154 0.277 0.369
KER 0.455 0.304 0.210 - 0.318 0.335
TieNet 0.330 0.194 0.124 0.081 - 0.311
CARG 0.359 0.237 0.164 0.113 - 0.354
SAT 0.433 0.281 0.194 0.138 0.320 0.361
SentSAT 0.445 0.289 0.200 0.143 0.268 0.359
SentSAT+KG (without classification) 0.456 0.299 0.213 0.159 0.37 0.36
SentSAT+KG (with classification) 0.431 0.275 0.189 0.135 0.190 0.345
SentSAT+KG (abnormalities only) 0.25 0.14 0.09 0.07 0.06 0.28
Table 2: Results for report generation. All models are trained and tested for both normal and abnormal cases except
the last line’s model. Notice that we only trained and tested the last one on abnormal cases.
Figure 4: Attention Map over Graph for a normal case
via SentSAT+KG (with classification).
6 Conclusions
In this project, we reviewed the (Zhang et al., 2020)
and showed how their results are actually overfitting to the normal cases though obtained relatively high metrics scores because of the very unbalanced data and the unsatisfying classification
performance. We added two supplementary experiments to illustrate the significance of having finding
classification before generation and to explore if the
model can avoid overfitting if only trained with abnormalities. For future work, instead of only training on abnormal cases, we believe it makes more
sense to first train for both normal and abnormal
cases for several epochs and then fine-tune it only
on abnormal cases since the model should learn
the general structure of the report first before predicting for abnormalities. To better understand the
model performance, we also demonstrated where
each generated sentence is focusing on by showing the feature maps over the graph and found that
the model didn’t pay attention to the hypothesized
areas. we encourage researchers to design more
suitable metrics for radiology report generation,
and to focus on more clinical informative report
generation instead of just reporting traditional evaluation metrics.
References
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and ´
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325.
Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer
Antani, George R Thoma, and Clement J McDonald. 2016. Preparing a collection of radiology examinations for distribution and retrieval. Journal
of the American Medical Informatics Association,
23(2):304–310.
Desmond Elliott and Frank Keller. 2013. Image description using visual dependency representations.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages
1292–1302.
Figure 5: Report Generation-A normal Case and an abnormal cases. The upper two is trained and tested on normal
and abnormal cases following the original settings while the bottom one is trained and tested only on the abnormal
cases.
Danna Gurari, Yinan Zhao, Meng Zhang, and
Nilavra Bhattacharya. 2020. Captioning images
taken by people who are blind. arXiv preprint
arXiv:2002.08565.
Baoyu Jing, Pengtao Xie, and Eric Xing. 2017. On
the automatic generation of medical imaging reports.
arXiv preprint arXiv:1711.08195.
Pavel Kisilev, Eli Sason, Ella Barkan, and Sharbell
Hashoul. 2016. Medical image description using
multi-task-loss cnn. In Deep Learning and Data
Labeling for Medical Applications, pages 121–129.
Springer.
Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P
Xing. 2019. Knowledge-driven encode, retrieve,
paraphrase for medical image report generation. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6666–6673.
Yuan Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing.
2018. Hybrid retrieval-generation reinforced agent
for medical image report generation. In Advances in
neural information processing systems, pages 1530–
1540.
Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag, Wei-Hung Weng, Peter
Szolovits, and Marzyeh Ghassemi. 2019. Clinically accurate chest x-ray report generation. arXiv
preprint arXiv:1904.02633.
Maram Mahmoud A Monshi, Josiah Poon, and Vera
Chung. 2019. Convolutional neural network to detect thorax diseases from multi-view chest x-rays.
In International Conference on Neural Information
Processing, pages 148–158. Springer.
Maram Mahmoud A Monshi, Josiah Poon, and Vera
Chung. 2020. Deep learning in generating radiology reports: A survey. Artificial Intelligence in
Medicine, page 101878.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for computational linguistics, pages 311–318. Association for
Computational Linguistics.
Thibault Sellam, Dipanjan Das, and Ankur P Parikh.
2020. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696.
Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina DemnerFushman, Jianhua Yao, and Ronald M Summers.
2016. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2497–2506.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 4566–4575.
Cheng Wang, Haojin Yang, Christian Bartz, and
Christoph Meinel. 2016. Image captioning with
deep bidirectional lstms. In Proceedings of the 24th
ACM international conference on Multimedia, pages
988–997.
Hongyu Wang and Yong Xia. 2018. Chestnet: A
deep neural network for classification of thoracic
diseases on chest radiography. arXiv preprint
arXiv:1807.03058.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and
Ronald M Summers. 2018. Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 9049–9058.
Yuan Xue, Tao Xu, L Rodney Long, Zhiyun Xue,
Sameer Antani, George R Thoma, and Xiaolei
Huang. 2018. Multimodal recurrent model with attention for automated radiology report generation.
In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages
457–466. Springer.
Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu,
Alan Yuille, and Daguang Xu. 2020. When radiology report generation meets knowledge graph.
arXiv preprint arXiv:2002.08277.
Yimin Zhou, Yiwei Sun, and Vasant Honavar. 2019.
Improving image captioning by leveraging knowledge graphs. In 2019 IEEE Winter Conference
on Applications of Computer Vision (WACV), pages
283–293. IEEE.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation
models. In The 41st International ACM SIGIR Conference on Research & Development in Information
Retrieval, pages 1097–1100.